{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class is for creating instances of Nodes \n",
    "class TreeNode:\n",
    "        \n",
    "    def __init__(self,level):\n",
    "        self.__level=level          # level of the node in tree structure\n",
    "        self.splittingFeature=\"\"    # on which feature the node will be splitted, if no further split then its value is null\n",
    "        self.__entropy=0.0          # entropy of the node\n",
    "        self.__gainRatio=-1         # gain ratio of the node\n",
    "        self.childList=[]           # this is list for storing the child nodes of the current node\n",
    "        self.result=\"\"              # used for storing the class which will be predicted on the basis of majority\n",
    "  \n",
    "\n",
    "\n",
    "    def addChild(self,node,data):   # method to add the child node and its data (splitted feature's value)\n",
    "        self.childList=self.childList+[(node,data)]              # appending the list\n",
    "#         self.childList.append((node,data))\n",
    "\n",
    "\n",
    "    def setSplittingFeature(self,f):       # method to set the splitting feature value\n",
    "        self.splitting_feature=f\n",
    "    \n",
    "    \n",
    "    def printCount(self,y):                # this method is used for printing the unique value (with their count) present \n",
    "                                           # in the output i.e. Y corresponding to data of current node\n",
    "        \n",
    "        dict={\"0\":\"setosa\",\"1\":\"versicolor\",\"2\":\"virginica\"} # dictionary storing the actual value of the output\n",
    "        \n",
    "        a=y.value_counts(ascending=True)                     # finding the unique values with their count in output\n",
    "        \n",
    "        for i in range(a.shape[0]):                          # printing the unique values with their count\n",
    "            print(\"Count of \",str(a.index[i]),\"(\",dict[str(a.index[i]).strip()],\")\",a.values[i])\n",
    "        \n",
    "        self.result=str(a.index[a.shape[0]-1]) # setting class to be predicted for node to class/value whose count is maximum\n",
    "            \n",
    "            \n",
    "    def setEntropy(self,entropy):           # method to set entropy \n",
    "        self.__entropy=entropy\n",
    "    \n",
    "    \n",
    "    def setGainRatio(self,gainRatio):       # method to set gain ratio \n",
    "        self.__gainRatio=gainRatio\n",
    "    \n",
    "    \n",
    "    def printDetails(self,y):               # method to print the details (that are various fields) of the node\n",
    "        \n",
    "        dict={\"0\":\"setosa\",\"1\":\"versicolor\",\"2\":\"virginica\"}           # dictionary storing the actual value of the output\n",
    "        print(\"Level\",self.__level)\n",
    "        self.printCount(y)\n",
    "        print(\"Current Entropy  is =\",self.__entropy)\n",
    "        \n",
    "        # checking whether the node is splitted further or not\n",
    "        \n",
    "        if(self.splittingFeature!=\"\"):     \n",
    "            print(\"Splitting on feature\",self.splittingFeature,\"with gain ratio\",self.__gainRatio)\n",
    "        else:\n",
    "            print(\"Reached leaf Node\")\n",
    "        print(\"Class to be predicted\",self.result,\"(\",dict[str(self.result).strip()],\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def gain(x, y, f):                       # method to calculate the gain ratio, \n",
    "                                         # Arguments are:- x : pandas dataframe, y : pandas series, f:list containing features  \n",
    "    \n",
    "    labels=getLabels(x[f])               # getting unique labels corresponding to the feature \n",
    "    x[\"y\"]=y                             # adding new column i.e output y in x dataframe\n",
    "\n",
    "    split_info_ = split_info(x[f])       # calling function to calculate the split info of the feature\n",
    "    \n",
    "    a=x[\"y\"].value_counts()              # finding the unique values of the output data\n",
    "    length_y=x.shape[0]                 \n",
    "    \n",
    "    entropy=info(a,length_y)        # for calculating the information of the feature i.e entropy\n",
    "    \n",
    "    # Calculating the Info gain\n",
    "    \n",
    "    new_info=0\n",
    "    for i in labels:                     # used for info gain after splitting to different nodes\n",
    "        new_x=x[x[f]==i]\n",
    "        a1=new_x[\"y\"].value_counts()\n",
    "        length_y1=new_x.shape[0]\n",
    "        \n",
    "        info_i=info(a1,length_y1)\n",
    "        \n",
    "        new_info+=float(new_x.shape[0]*info_i)/float(x.shape[0])\n",
    "        \n",
    "    info_gain=entropy-new_info                           # info gain\n",
    "    \n",
    "    if(split_info_==0):\n",
    "        return info_gain\n",
    "    \n",
    "    gain_ratio=info_gain/split_info_                     # gain-ratio\n",
    "\n",
    "    return (entropy,gain_ratio)\n",
    "    \n",
    "    \n",
    "def split_info(x):                                       # method to calculate split ; Argument x : pandas dataframe\n",
    "    split_info=0                                           \n",
    "    a=x.value_counts()                                   # for getting unique values with their count\n",
    "    \n",
    "    for i in range(a.shape[0]):\n",
    "        p=float(a.values[i])/float(x.shape[0])           # caculating the probability of each splitted node\n",
    "        log=math.log(p,10)\n",
    "        split_info += p*log\n",
    "    split_info= (-1)*split_info\n",
    "    \n",
    "    return split_info\n",
    "\n",
    "def info(a,length_y):                                    # method to calculate information corresponding to a node\n",
    "                                                         # Arguments :- a : pandas series, length_y: int\n",
    "\n",
    "    info_i=0\n",
    "        \n",
    "    for i in range(a.shape[0]):\n",
    "        p=float(a.values[i])/float(length_y)\n",
    "        log=math.log(p,10)\n",
    "        info_i += p*log\n",
    "        \n",
    "    info_i= (-1)*info_i\n",
    "    \n",
    "    return info_i\n",
    "    \n",
    "def getLabels(x):                                        # method for finding each unique label corresponding to the feature\n",
    "                                                         # x : pandas dataframe\n",
    "    \n",
    "    a=x.value_counts()                                   # for getting unique values with their count\n",
    "    length=a.shape[0]\n",
    "    labels=[]\n",
    "    for i in range(a.shape[0]):\n",
    "        labels.append(a.index[i])\n",
    "    return labels\n",
    "\n",
    "\n",
    "def decisionTree(x, y, level, features):                # Method to build the decision tree\n",
    "                                                        # Arguments:- x: pandas dataframe, y : pandas series\n",
    "                                                        #   level: int, features: list containing features' name\n",
    "    print()\n",
    "    \n",
    "    node=TreeNode(level)                                # Creating the Tree Node object \n",
    "    \n",
    "    a=set()                                             # creating set for finding the unique values of y\n",
    "    \n",
    "    for i in range(y.shape[0]):                         # adding the value of output i.e y in the set\n",
    "        a.add(y.iloc[i])      \n",
    "        \n",
    "    if(len(a)==1):                      # if the set contains single value i.e. their is only one value of output possible \n",
    "                                        # means pure node is achieved\n",
    "        node.printDetails(y)            # calling node method for printing the details of the node\n",
    "        return node                     # returning as further no split is possible\n",
    "\n",
    "    elif(len(features)<=0):             # checking whether the features are available \n",
    "                                        # so no more features to split upon as features array is empty\n",
    "            \n",
    "        a=y.value_counts()              # calculating the different labels with thier count of the output corresponding to the  \n",
    "                                        # node as this may not be a pure node and if not pure node, majority will decide output \n",
    "        \n",
    "        length_y=x.shape[0]\n",
    "\n",
    "        info_feature=info(a,length_y)                   # calculating the information of the feature i.e the entropy \n",
    "        node.setEntropy(float(info_feature))            # setting the entropy value of the node\n",
    "        node.printDetails(y)                            # calling node's method for printing the details of the node\n",
    "        return node\n",
    "    \n",
    "    \n",
    "    #### Finding the best feature to split upon, feature with maximum gain\n",
    "    \n",
    "    max_gain=-1\n",
    "    max_entropy=-1\n",
    "    best_feature=\"\"\n",
    "    \n",
    "    for f in features:                  # calculating gain ratio of each feature\n",
    "        g1=gain(x, y, f)\n",
    "        g=g1[1]\n",
    "        entropy=g1[0]\n",
    "        if(g>max_gain):                 # updating the best feature according to the maximum gain\n",
    "            best_feature=f\n",
    "            max_gain=g\n",
    "            max_entropy=entropy\n",
    "            \n",
    "    features.remove(best_feature)       # removing the current feature so that it may not be used again for splitting\n",
    "    \n",
    "    #### setting different fields of the current Node\n",
    "    node.setSplittingFeature(best_feature)\n",
    "    node.splittingFeature=best_feature\n",
    "    node.setEntropy(max_entropy)\n",
    "    node.setGainRatio(max_gain)\n",
    "    node.printDetails(y)                # calling node's method for printing the details of the node\n",
    "    \n",
    "    \n",
    "    #### Recursion to build the complete tree\n",
    "    \n",
    "    labels=getLabels(x[best_feature])         # getting the different labels corresponding to the selected feature\n",
    "    \n",
    "    x[\"y\"]=y                                  # adding the output column i.e. y to x dataframe so that it is easy to split the \n",
    "                                              # output y corresponding to the split of x dataframe  \n",
    "    \n",
    "    for i in labels:                          # iterating over each label so to build new nodes corresponding to each label\n",
    "        \n",
    "        new_x = x[x[best_feature]==i]   # getting x dataframe where the value of feature (column) is equal to the current label\n",
    "        new_y = new_x[\"y\"]                    # getting the new y for corresponding x dataframe\n",
    "        new_x.drop('y',axis=1,inplace=True)   # droping the output column from 'new_x' dataframe as we have output in \n",
    "                                              #'new_y' variable\n",
    "    \n",
    "        #### recursive step to build nodes\n",
    "        node.addChild(decisionTree(new_x, new_y, level+1, features),i)\n",
    "    \n",
    "    x.drop('y',axis=1,inplace=True)           # droping the output column form x dataframe as we have output stored in y\n",
    "    return node                               # this method returns the node\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x,y,features):                          # method for training the algorithm using training input (x : pandas dataframe)\n",
    "                                                # and output (y : pandas series),also the list of features (features' list)\n",
    "        \n",
    "    return decisionTree(x, y, 0, features)      # building decision tree\n",
    "    \n",
    "    \n",
    "def predictValue(node,x):                        # method to predict the output corresponding to a single row of x dataframe\n",
    "                                                 # Arguments:- node: TreeNode, x : single row of pandas dataframe \n",
    "    \n",
    "    if (len(node.childList)==0):                 # base case, when node is not further splitted\n",
    "        return node.result                       # so we return the majority value of the node as a result, \n",
    "                                                 # this is stored in node's field \"result\"\n",
    "        \n",
    "    x_data=x[node.splittingFeature]              # getting the column corresponding to the splitted feature\n",
    "    x_data=x_data.iloc[0]                        # getting the value in the column, as this is a single row\n",
    "    \n",
    "    for i in range(len(node.childList)):         # for finding to which label the value of our data ('x_data' variable) belongs\n",
    "                                                 # by looking at each child nodes (labels corresponding to splitting feature) of\n",
    "                                                 # the node \n",
    "                \n",
    "        value=node.childList[i]                  # getting the label \n",
    "        curr_node=value[0]                       # getting the node of the label\n",
    "        curr_node_data=value[1]                  # getting the value of the label\n",
    "                    \n",
    "        if(x_data==curr_node_data):              # comparing our data with the label value\n",
    "            return predictValue(curr_node,x)     # using recursion, and sending the node of the label whose value matches \n",
    "                                                 # our data\n",
    "    \n",
    "def predict(node,x):                     # method to predict the output corresponding to dataframe x\n",
    "                                         # Arguments:- node: TreeNode, x : complete pandas dataframe\n",
    "\n",
    "    y=[]                                 # used for storing output corresponding to each row of x\n",
    "    \n",
    "    for i in range(x.shape[0]):          # finding output corresponding to each row of x\n",
    "        a=x.iloc[i:]                     # getting a row of x\n",
    "        result=predictValue(node,a)      # predicting the value corresponding to this row by calling the 'predictValue' function\n",
    "        y.append(result)                 # adding the result to our output array\n",
    "    Y=np.array(y)                        # converting the array to numpy array\n",
    "    y = pd.Series(Y)                     # converting the numpy array to pandas series\n",
    "    return y\n",
    "        \n",
    "def score(y_pred,y_true):                   # method for calculating the score \n",
    "                                            # Arguments:- y_pred: pandas series, y_true: pandas series \n",
    "    count=0                                 # stores the number of values where the output predicted is same as actual output\n",
    "    for i in range(y_true.shape[0]):        # iterating over each row of predicted and actual output\n",
    "        a=y_true.iloc[i]                    # value of the actual output for the current row\n",
    "        b=y_pred.iloc[i]                    # value of the predicted output for the current row\n",
    "        \n",
    "        if(str(a).strip()==str(b).strip()):   # if the actual and predicted output are same the count is increased by 1\n",
    "            count+=1\n",
    "            \n",
    "    score=count/y_true.shape[0]         # calculating the score, score = (no of correct values predicted)/(total no of values)\n",
    "    return score\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;30m----------------------------------- Actual Iris Data -----------------------------------\u001b[0m\n",
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                  6.5               2.8                4.6               1.5\n",
      "1                  6.7               2.5                5.8               1.8\n",
      "2                  6.8               3.0                5.5               2.1\n",
      "3                  5.1               3.5                1.4               0.3\n",
      "4                  6.0               2.2                5.0               1.5\n",
      "..                 ...               ...                ...               ...\n",
      "107                6.3               2.8                5.1               1.5\n",
      "108                6.4               3.1                5.5               1.8\n",
      "109                6.3               2.5                4.9               1.5\n",
      "110                6.7               3.1                5.6               2.4\n",
      "111                4.9               3.6                1.4               0.1\n",
      "\n",
      "[112 rows x 4 columns]\n",
      "\n",
      "\u001b[1;30;30m----------- Iris Data after converting the Continous Data to Discrete Classes -----------\u001b[0m\n",
      "    sepal_length sepal_width petal_length petal_width\n",
      "0        class 3     class 0      class 2     class 2\n",
      "1        class 3     class 0      class 3     class 2\n",
      "2        class 3     class 1      class 3     class 3\n",
      "3        class 0     class 3      class 0     class 0\n",
      "4        class 2     class 0      class 2     class 2\n",
      "..           ...         ...          ...         ...\n",
      "107      class 2     class 0      class 2     class 2\n",
      "108      class 2     class 2      class 3     class 2\n",
      "109      class 2     class 0      class 2     class 2\n",
      "110      class 3     class 2      class 3     class 3\n",
      "111      class 0     class 3      class 0     class 0\n",
      "\n",
      "[112 rows x 4 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris=datasets.load_iris()                   # getting the iris data\n",
    "x_train,x_test,y_train,y_test= train_test_split(iris.data,iris.target,random_state=1) # splitting into testing and training data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "BOLD = '\\x1b[1;30;30m'\n",
    "END = '\\x1b[0m'\n",
    "\n",
    "def get_x(x_train,boolean_Data):                    # to get x which can be passed to our decision tree\n",
    "                                                    # boolean_Data is for printing purposes whether to print x dataframe or not\n",
    "                                                    # Arguments:- x: numpy n-dim array, boolean_Data: True/False\n",
    "    \n",
    "    features=['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']    # feature's name \n",
    "    x = pd.DataFrame(x_train,columns=features)        # converting the x data to pandas dataframe\n",
    "    \n",
    "    if(boolean_Data):\n",
    "        # printing the iris data\n",
    "        print(BOLD+\"----------------------------------- Actual Iris Data -----------------------------------\"+END)          \n",
    "        print(x)\n",
    "        print()\n",
    "    \n",
    "    def fun(s):                             # method to convert string value (contains real number) to float \n",
    "                                            # Argument:- s: string\n",
    "        s=str(s)\n",
    "        return float(s)\n",
    "\n",
    "    def converting_to_classes(x):           # method to convert continous data into discrete data, attribute is a pandas series\n",
    "                                            # Argument x: pandas dataframe\n",
    "        \n",
    "        b=x.apply(fun)                      # converting string value to float \n",
    "\n",
    "        a=b.copy()\n",
    "        a=a.to_numpy(dtype=float)           # converting the pandas series to numpy array\n",
    "        a.sort()                              # sorting the data of numpy array\n",
    "        size=a.shape[0]                     # size of the numpy array\n",
    "        \n",
    "        #### the data (contionus) is splitted into 4 descrete values for which 3 boundaries are required\n",
    "        first_split = a[int(size/4)]            # first boundary \n",
    "        second_split = a[int((2*size)/4)]       # second boundary \n",
    "        third_split = a[int((3*size)/4)]        # third boundary \n",
    "        \n",
    "        result_array=[]                 # stores the discrete value (4 possibilites) corresponding to the each continous data\n",
    "        \n",
    "        for i in range(b.shape[0]):     # iterating over each row of the pandas series\n",
    "            \n",
    "            if(b[i]<=first_split):              # before first boundary, so the continous value is converted to class \"0\"\n",
    "                result_array.append(\"class 0\")\n",
    "            \n",
    "            elif (first_split<b[i] and b[i]<=second_split):  # between first and second boundary, so the continous value is \n",
    "                                                             # converted to class \"1\"\n",
    "                result_array.append(\"class 1\")\n",
    "                \n",
    "            elif (second_split<b[i] and b[i]<=third_split):  # between second and third boundary, so the continous value is \n",
    "                                                             # converted to class \"2\"\n",
    "                result_array.append(\"class 2\")\n",
    "                \n",
    "            else:                               # after fourth boundary, so the continous value is converted to class \"3\"\n",
    "                result_array.append(\"class 3\")\n",
    "        result_array = np.array(result_array)   # result array is converted to numpy array     \n",
    "        result_array = pd.Series(result_array)  # result array is converted to pandas series \n",
    "\n",
    "        return result_array\n",
    "\n",
    "    #### Continous data of each column is converted to discrete classes\n",
    "    x[\"sepal_length\"]=converting_to_classes(x[\"sepal length (cm)\"])          \n",
    "    x[\"sepal_width\"]=converting_to_classes(x[\"sepal width (cm)\"])\n",
    "    x[\"petal_length\"]=converting_to_classes(x[\"petal length (cm)\"])\n",
    "    x[\"petal_width\"]=converting_to_classes(x[\"petal width (cm)\"])\n",
    "    x.drop(features,axis=1,inplace=True)                                     # continous data is droped\n",
    "    \n",
    "    if(boolean_Data):\n",
    "        # printing the iris data after converting the Continous Data to Discrete Classes\n",
    "        print(BOLD+\"----------- Iris Data after converting the Continous Data to Discrete Classes -----------\"+END)\n",
    "        print(x)\n",
    "        print()\n",
    "    \n",
    "    return x\n",
    "x = get_x(x_train,True)                     # getting the x dataframe which can be passed to decision tree\n",
    "y = pd.Series(y_train)                      # converting the y data to pandas series, so that it can be passed to decision tree\n",
    "\n",
    "\n",
    "features=[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]     # features to be passed to decision tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Level 0\n",
      "Count of  1 ( versicolor ) 34\n",
      "Count of  0 ( setosa ) 37\n",
      "Count of  2 ( virginica ) 41\n",
      "Current Entropy  is = 0.47584404860389007\n",
      "Splitting on feature petal_length with gain ratio 0.5975745899049689\n",
      "Class to be predicted 2 ( virginica )\n",
      "\n",
      "Level 1\n",
      "Count of  0 ( setosa ) 33\n",
      "Current Entropy  is = 0.0\n",
      "Reached leaf Node\n",
      "Class to be predicted 0 ( setosa )\n",
      "\n",
      "Level 1\n",
      "Count of  1 ( versicolor ) 13\n",
      "Count of  2 ( virginica ) 15\n",
      "Current Entropy  is = 0.2999211575627872\n",
      "Splitting on feature petal_width with gain ratio 0.2865660631216426\n",
      "Class to be predicted 2 ( virginica )\n",
      "\n",
      "Level 2\n",
      "Count of  2 ( virginica ) 8\n",
      "Count of  1 ( versicolor ) 11\n",
      "Current Entropy  is = 0.2955936308119856\n",
      "Splitting on feature sepal_length with gain ratio 0.2720812438497109\n",
      "Class to be predicted 1 ( versicolor )\n",
      "\n",
      "Level 3\n",
      "Count of  1 ( versicolor ) 6\n",
      "Count of  2 ( virginica ) 7\n",
      "Current Entropy  is = 0.2997438305836322\n",
      "Splitting on feature sepal_width with gain ratio 0.13702530817459677\n",
      "Class to be predicted 2 ( virginica )\n",
      "\n",
      "Level 4\n",
      "Count of  1 ( versicolor ) 3\n",
      "Count of  2 ( virginica ) 4\n",
      "Current Entropy  is = 0.296583221518423\n",
      "Reached leaf Node\n",
      "Class to be predicted 2 ( virginica )\n",
      "\n",
      "Level 4\n",
      "Count of  1 ( versicolor ) 1\n",
      "Count of  2 ( virginica ) 3\n",
      "Current Entropy  is = 0.24421905028821553\n",
      "Reached leaf Node\n",
      "Class to be predicted 2 ( virginica )\n",
      "\n",
      "Level 4\n",
      "Count of  1 ( versicolor ) 1\n",
      "Current Entropy  is = 0.0\n",
      "Reached leaf Node\n",
      "Class to be predicted 1 ( versicolor )\n",
      "\n",
      "Level 4\n",
      "Count of  1 ( versicolor ) 1\n",
      "Current Entropy  is = 0.0\n",
      "Reached leaf Node\n",
      "Class to be predicted 1 ( versicolor )\n",
      "\n",
      "Level 3\n",
      "Count of  1 ( versicolor ) 5\n",
      "Current Entropy  is = 0.0\n",
      "Reached leaf Node\n",
      "Class to be predicted 1 ( versicolor )\n",
      "\n",
      "Level 3\n",
      "Count of  2 ( virginica ) 1\n",
      "Current Entropy  is = 0.0\n",
      "Reached leaf Node\n",
      "Class to be predicted 2 ( virginica )\n",
      "\n",
      "Level 2\n",
      "Count of  2 ( virginica ) 7\n",
      "Current Entropy  is = 0.0\n",
      "Reached leaf Node\n",
      "Class to be predicted 2 ( virginica )\n",
      "\n",
      "Level 2\n",
      "Count of  1 ( versicolor ) 2\n",
      "Current Entropy  is = 0.0\n",
      "Reached leaf Node\n",
      "Class to be predicted 1 ( versicolor )\n",
      "\n",
      "Level 1\n",
      "Count of  2 ( virginica ) 26\n",
      "Current Entropy  is = 0.0\n",
      "Reached leaf Node\n",
      "Class to be predicted 2 ( virginica )\n",
      "\n",
      "Level 1\n",
      "Count of  0 ( setosa ) 4\n",
      "Count of  1 ( versicolor ) 21\n",
      "Current Entropy  is = 0.19094620248307143\n",
      "Reached leaf Node\n",
      "Class to be predicted 1 ( versicolor )\n"
     ]
    }
   ],
   "source": [
    "\n",
    "root = fit(x, y, features)                        # using the x and y data to build the decision tree, fit method will return \n",
    "                                                  # the root node of the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE of training data \u001b[1;30;30m 0.9285714285714286 \u001b[0m\n",
      "SCORE of testing data \u001b[1;30;30m 0.8157894736842105 \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "y_pred=predict(root,x)                            # predicting the output corresponding the x, note x here is same as training x\n",
    "                                                  # but is already converted to required form using 'get_x' method\n",
    "print(\"SCORE of training data\",BOLD,score(y_pred,y),END)   # printing the training data score by calling the 'score' method\n",
    "\n",
    "\n",
    "#### Checking the algorithm by testing it on the testing data\n",
    "\n",
    "x1 = get_x(x_test,False)  # getting x_test which can be passed to the decision tree (converting continous data to discrete data)\n",
    "y1 = pd.Series(y_test)    # actual output is converted pandas series\n",
    "\n",
    "y_pred_test=predict(root,x1)    # predicting the output of the testing input by passing the root of decision tree and input data\n",
    "print(\"SCORE of testing data\",BOLD,score(y_pred_test,y1),END)  # printing the testing data score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both training and testing score are good, so the decision tree algorithm is performing well, also splitting the continous data into 4 classes assures that decision tree is not too big or too short, so, in descent amount of time the prediction is done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
